{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine - Quality - Classification\n",
    "\n",
    "- Ke Deng\n",
    "- Dataset: Wine Dataset\n",
    "\n",
    "# 1 - Data Processing Phase\n",
    "Import Python packages we will use in this project.\n",
    "## 1.1 - Download and Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15708/656700723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display \n",
    "from statistics import *\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, precision_recall_fscore_support, accuracy_score \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download data if they are not in current directory\n",
    "if not os.path.isfile('winequality-red.csv'):\n",
    "    wget.download(url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv')\n",
    "    \n",
    "#read data from files\n",
    "data = pd.read_csv('winequality-red.csv',sep = ';')\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "data['class'] = [1 if i > 5 else 0 for i in data['quality']]\n",
    "data['class'] = data['class'].astype('float64')\n",
    "data.drop(columns = ['quality'], inplace = True)\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thre is no missing value in this dataset.\n",
    "## 1.2 - Normalization \n",
    "We use min-max normalization to normalize whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['class'], axis=1).copy()\n",
    "y = data['class']\n",
    "# X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "# Normalization\n",
    "# Fitting only on training data\n",
    "\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df = pandas.DataFrame(x_scaled)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = pd.DataFrame(min_max_scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Models\n",
    "We firstly define a function which returns accuracy, true positive rate, false positive rate, precision, and F-score in binary classification. In this case, we use 10-fold cross validation, so there is no need to split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(test, pred):\n",
    "    tp = tn = fp = fn = 0\n",
    "    for i in range(len(test)):\n",
    "        if pred[i] == 1 and test[i] == 1:\n",
    "            tp = tp + 1\n",
    "        elif pred[i] == 0 and test[i] == 0:\n",
    "            tn = tn + 1\n",
    "        elif pred[i] == 1 and test[i] == 0:\n",
    "            fp = fp + 1\n",
    "        else:\n",
    "            fn = fn + 1\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    true_pos_rate = tp/(tp + fn)\n",
    "    false_pos_rate = fp/(fp + tn)\n",
    "    precision = tp/(tp + fp)\n",
    "    f_score = 2*true_pos_rate*precision/(true_pos_rate + precision)\n",
    "    return accuracy, true_pos_rate, false_pos_rate, precision, f_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Logistic Regression\n",
    "The first model we choose is logistic regression. \n",
    "- Use default L2 penalties, which outperforms L1 with higher accuracy \n",
    "- We can not see very obvious improvement on accuracy by increasing times of iteration, therefore, we set iteration times as its default value 100. After training this model using training dataset \n",
    "\n",
    "We use We use 10-fold corss validation to exam the model and gain an average accuracy. Following table shows the accuracy, TPR, FPR, precision, F-score on validation dataset. The figure shows ROC curve given by each iteration, the grey area stands for average ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "log_reg_cfsn_matrix = pd.DataFrame(columns = ('Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score'))\n",
    "\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    log_reg_pred = log_reg.predict(X_test)\n",
    "    log_reg_prob = log_reg.predict_proba(X_test)\n",
    "    \n",
    "    y_test_temp = y_test.tolist()\n",
    "    log_reg_cfsn_matrix.loc[len(log_reg_cfsn_matrix)] = my_confusion_matrix(y_test_temp, log_reg_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, log_reg_prob[:,1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "    tpr = interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "plt.plot(base_fpr, mean_tprs, 'b')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "\n",
    "print(\"Average accuracy of logistic regression model is %0.4f\" % mean(log_reg_cfsn_matrix['Accuracy']))\n",
    "log_reg_cfsn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Support Vector Machine\n",
    "The second model we choose is support vector machine.\n",
    "- Set kernel as rbf, wich outperformes 'linear, 'poly' and 'sigmoid' kernels\n",
    "- Set C as 1, which gives the highest accuracy amongst 0.1, 0.5, 1 and 10\n",
    "- Set tolerance for stopping criteria as 1e-3, which gives the highest accuracy amongst 1e-1, 1e-2, 1e-3, 1e-4 and 1e-5\n",
    "\n",
    "We use 10-fold corss validation to exam the model and gain an average accuracy. The figure shows ROC curve given by each iteration, the grey area stands for average ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel = 'rbf', C = 1, tol = 1e-3, probability=True)\n",
    "# kf = KFold(n_splits=10)\n",
    "# kf.get_n_splits(X)\n",
    "# KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "svc_cfsn_matrix = pd.DataFrame(columns = ('Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score'))\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    svc.fit(X_train, y_train)\n",
    "    svc_pred = svc.predict(X_test)\n",
    "    svc_prob = svc.predict_proba(X_test)\n",
    "    \n",
    "    y_test_temp = y_test.tolist()\n",
    "    svc_cfsn_matrix.loc[len(svc_cfsn_matrix)] = my_confusion_matrix(y_test_temp, svc_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, svc_prob[:,1]) \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "    \n",
    "    # to plot average area\n",
    "    tpr = interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "plt.plot(base_fpr, mean_tprs, 'b')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "\n",
    "print(\"Average accuracy of support vector machine is %0.4f\" % mean(svc_cfsn_matrix['Accuracy']))\n",
    "svc_cfsn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - Decision Tree\n",
    "The third model we choose is decision tree. We set gini index as criterion. \n",
    "- Set Gini Index as spliting criteria, which outperformes Entropy with a higher accuracy\n",
    "- Set min samples required to split as 5%, which gives the highest accuracy amongst 1%, 5%, and 10%\n",
    "- Set min samples required at leaf as 0.1 % which gives the highest accuracy amongst 0.1%, 1%, 5%\n",
    "\n",
    "We use 10-fold corss validation to exam the model and gain an average accuracy. The figure shows ROC curve given by each iteration, the grey area stands for average ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "# kf = KFold(n_splits=10)\n",
    "# kf.get_n_splits(X)\n",
    "# KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "clf_cfsn_matrix = pd.DataFrame(columns = ('Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score'))\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    clf_pred = clf.predict(X_test)\n",
    "    clf_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    y_test_temp = y_test.tolist()\n",
    "    clf_cfsn_matrix.loc[len(clf_cfsn_matrix)] = my_confusion_matrix(y_test_temp, clf_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, clf_prob[:,1]) \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "    # to plot average area\n",
    "    tpr = interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "plt.plot(base_fpr, mean_tprs, 'b')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "\n",
    "print(\"Average accuracy of decision tree model is %0.4f\" % mean(clf_cfsn_matrix['Accuracy']))\n",
    "clf_cfsn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Random Forest\n",
    "The fourth model we choose is random forest. \n",
    "\n",
    "- Set number of estimators as 100, which is the best amongst 10, 50 and 100\n",
    "- Set splitting criteria as Gini Index \n",
    "- Set min samples required to split: 5% \n",
    "- Set min samples required at leaf as 0.1% \n",
    "\n",
    "We use 10-fold corss validation to exam the model and gain an average accuracy. The figure shows ROC curve given by each iteration, the grey area stands for average ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_forest = RandomForestClassifier(n_estimators=100, criterion = 'gini', \n",
    "                                  max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "# kf = KFold(n_splits=10)\n",
    "# kf.get_n_splits(X)\n",
    "# KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "forest_cfsn_matrix = pd.DataFrame(columns = ('Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score'))\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    r_forest.fit(X_train, y_train)\n",
    "    r_forest_pred = r_forest.predict(X_test)\n",
    "    r_forest_prob = r_forest.predict_proba(X_test)\n",
    "    \n",
    "    y_test_temp = y_test.tolist()\n",
    "    forest_cfsn_matrix.loc[len(forest_cfsn_matrix)] = my_confusion_matrix(y_test_temp, r_forest_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, r_forest_prob[:,1]) \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "    # to plot average area\n",
    "    tpr = interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "plt.plot(base_fpr, mean_tprs, 'b')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "\n",
    "print(\"Average accuracy of random forest model is %0.4f\" % mean(forest_cfsn_matrix['Accuracy']))\n",
    "forest_cfsn_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model Evaluation\n",
    "## 3.1 - Overall Performance Statistics\n",
    "We aggregate average performence statistics into one talbe and plot average ROC curve in one figure to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_eval = pd.concat([log_reg_cfsn_matrix.mean(axis=0), \n",
    "                          svc_cfsn_matrix.mean(axis=0), clf_cfsn_matrix.mean(axis=0), \n",
    "                          forest_cfsn_matrix.mean(axis=0)], axis = 1)\n",
    "overall_eval.reset_index(inplace = True, drop = True)\n",
    "overall_eval.columns = ['Logistic Regression', 'SVM','Decision Tree', 'Random Forest']\n",
    "overall_eval.index = ['Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score']\n",
    "overall_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Conclusion\n",
    "- Randon forest model gives the highest F-score and accuracy.\n",
    "- The average ROC curve given by random forest model is close to top left corner\n",
    "\n",
    "So we choose random forest model as our final model. The advantage of choosing random forest is that it is very easy to measure the relative importance of each feature on the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Prediction\n",
    "Use logistic regression model to make prediction on our test dataset. We newly split our data into training and test dataset. And train a random forest model then deploy it to make prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "r_forest = RandomForestClassifier(n_estimators=100, criterion = 'gini', \n",
    "                                  max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "test_matrix = pd.DataFrame(columns = ('Accuracy', 'True Positive Rate', 'False Positive Rate', 'Precision', 'F-Score'))\n",
    "r_forest.fit(X_train, y_train)\n",
    "r_forest_pred = r_forest.predict(X_test)\n",
    "r_forest_prob = r_forest.predict_proba(X_test)\n",
    "    \n",
    "y_test_temp = y_test.tolist()\n",
    "test_matrix.loc[len(test_matrix)] = my_confusion_matrix(y_test_temp, r_forest_pred)\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(y_test, r_forest_prob[:,1]) \n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
